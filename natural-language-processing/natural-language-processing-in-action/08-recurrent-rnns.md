# Recurrent Neural Netowrks

---

* Concept of memory in a neural net
* Structure of recurrent neural net
* Data handling for RNNs
* Backpropagation Through Time (BPTT)

---

# given a neural network a rudimentary memory.

* Convolutional Neural Nets make an attempt to capture that ordering relationship, by capturing localized relationships, but there is another way.

* In standrad ANN, the effects of the learning stage of the next example are largely independent of the order of input data.

# Recurrent Neural Nets (RNNs)
 * are the first step toward being able to remember.
 * For each input we into a regular Feed Forward Net, we would like to take the output of the network at time step t and provide it as an additional input, along with the next piece of data being fed into the network at time step t+1.
 * tell the Feed Forward network what happened before along with what is happening "now".
 * As before we will tokenize the sentence. But where before we put those tokens into the network all at once, we will now put them in one at a time. 
 * This is not the same as having multiple new samples. The tokens are still part of one data sample with one associated label.
 * The tokens, in the order they appear in the sentence, will be the inputs at each time step. 
 * This segmentation (word level sentence segmentation at first and later at the character level) will occur for each individual data sample. 
 * The steps of putting each token in will be sub-steps of feeding the data sample into the network.
 * feedback is represented with an arc from the output of a layer back into it own inputs.
 *  unrolling the net
   * the hidden layerX output (or output layer?) of t-1, is used as input to the hidden layerX of t.
 * Each neuron in the hidden state has a set of weights that it applies to each of the elements of each input vector, as a normal Feed Forward network. 
 * But now we have an additional set of trainable weights that is applied to the output of the hidden neurons from the previous time step. 
 * The network can learn how much weight or importance to give the events of the "past" as we input a sequence token by token.
 * The first input in a sequence will have no "past" so the hidden state at t=0 receives an input of 0 from its t-1 self
 * we have a set of documents, each a labeled example. F
   * or each example, instead of passing the collection of word vectors into the net all at once as in the last chapter, we take the sample one token at a time.
 * In aRecurrent Neural Net, we pass in the word vector for the first token and get the network’s output. 
 * We then pass in the second token, but we also pass in the output from the first token! 
 * And then pass in the third token along with the output from the second token. And so on.

# Backpropagation Through Time
 * instead of dealing with the output generated by any of these "sub-samples" directly, we feed it back into the network. We are only concerned with the final output.
 * only last output matters.
 * We use the chain-rule to backpropagate to the next layer below. But instead of going to the layer below we go to the layer in the past, as if each unrolled version of the network were different. The math is the same.
 * Error is propogateted from the ouput, and back through the time-series hidden layers.

# RNN training backprop propgation summary
 * Break each data sample into tokens
 * Pass each token into a Feed Forward net
 * Pass the output of each time step as input into the same layer with the input from the next time step
 * Collect the output of the last time step and compare it to the label
 * Backpropagate the error through the whole graph, all the way back to the first input at time step 0.

# RSS Weight Update Process - Aggreagtates (last ouput matters)
 * The tricky part of the update process is the weights we are updating are not just each a different branch of a neural network. Each leg is the same network at different time steps. The weights are the same for each time step.
 * The changes are agrregated and applied back to the weights at t0.
 * In a Feed Forward network all of the weight updates would be calculated once the all of the gradients had been calculated for that input. Here the same holds, but we have to hold the updates until we go all the way back in time, to time step 0 for that particular input sample.
 * A weight at time step t contributed something to the error when it was initially calculated. That same weight received a different input at time step t+1 and therefor contributed a different amount to the error then.
 * we can figure out the various changes to the weights (as if they were in a bubble) at each time step and then sum up the changes and apply the aggregated changes to each of the weights of the hidden layer as the very last step of the learning phase.
 * As we backpropagate through time, a single weight may be pushed up at one time step t (given how it reacted to the input at time step t) and be pushed down at time step for t-1 (given how it reacted to the input at time step t-1), for a single data sample

 # RSS Weight Update Process - Intermediate Time steps (all outputs matter)
 * There are times where we may care about the entire sequence that is generated by each of the intermediary time steps as well.
 * Sometimes the output at a given time step t is just as import as the output at the final time step.
 * There is a way to capturing the error at any given time step and carrying that backwards in the backpropagation
 * This process is just like the normal backpropagation through time for n time steps.

# This is the most import take away of this section. As with a standard Feed Forward network: you always update the weights only after you have calculated the proposed change in the weights for the entire backpropagation step for that input (or set of inputs). In the case of Recurrent Neural Net, this includes the updates all the way back time t=0.

# RNN Properties
 * Recurrent Neural Net may have relatively fewer weights (parameters) to learn, you can see from the above how a recurrent neural can quickly get very expensive to train, especially for sequences of any length, say greater than 10 tokens.
 * Each step back in time is that many more derivatives to calculate.
 * Recurrent Neural Nets as they can take multivariate length input,

# vanishing gradient problem 
 * the vanishing gradient problem and it has a corollary the exploding gradient problem.
   * as a network gets deeper (more layers) the error signal has a hard time reaching the deepest layers.
 * Recurrent Neural Nets as each time step back in time is the mathematical equivalent of backpropagating an error "down" a layer in a Feed Forward network.
 * worse here - sentences maybe 10s or hundreds of token long.
 * mitigating factor thoug - While the gradient may vanish on the way to the last weight set, we are really only updating one weight set, which is the same at every "layer" or time step.

# Flatten Layer
 * Flatten(), that Keras provides to flatten the input from a tensor 400 x 50 to a vector 20,000 elements long.

# (keras) Hyperparameters
 * maxlen = 400 : Arbitrary sequence length based on perusing the data.
 * embedding_dims = 300 : This came from the pre-trained word2vec model
 * batch_size = 32 : Number of sample sequences to pass through (and aggregate the error) before backpropagating
 * epochs = 2
 * num_nuerons = 50 : Hidden layer complexity

# RNN vs CNN
 * Dont chose RNN - RNN is more expensive to commpute stuff.
 * RNN: problems of vanishing gradients are usually too much for a Recurrent Neural Net to overcome.
 * remembering bits of input that have already occurred is absolutely crucial in Natural Language Processing.
   * there are alternative ways to remember.

# Statefulness
 * when you want to remember from one input sample to the next, not just one time step to the next within a single sampl
 * Keras SimpleRNN 'stateful' option
   * when true: the last output of the last sample will be passed into itself at the next time step along with the first token input, just as would happen in the middle of the sample.
 * This can be useful when we try to model a larger document or the nature of the entire corpus itself. 
 * You would not use it in an example such as the one above. As the samples are each unrelated, the order of the samples themselves here provides no new information to the system.

# Bi-directional Recurrent Neural Nets
 * "They wanted the pet the dog whose fur was brown."
 * Keras has recently added a layer wrapper that will automatically flip and the necessary inputs and outputs and simultaneously calculate a bi-directional RNN for us.
 * The basic idea is you take two RNN’s right next to each other and pass the input into one as normal and pass the same input backwards into other net.
 * The output of those two are then concatenated at each time step to the related time step in the other network, related being same input token.
 * So we take the output the final time step in the input and concatenate it with the output that is generated by the same input token at the first time step of the backward net.
 * this can be useful as a Recurrent Neural Network (due to the vanishing gradient problem) is more receptive to data at the end of the sample than at the beginning.

# Summary
* In sequence data like language, what came before is important
* Splitting single data samples along the dimension of time can lead to deeper understanding
* We can backpropagate error back in time
* Weights are adjusted adjusted in aggregate across time for a given sample
* Gradients are temperamental and may disappear or explode
* There are different ways to examine the output of Recurrent Neural Nets
* Tuning Neural Nets is hard
* We can go backward and forward in time simultaneously





 ---

 Notes

* Give the two weight update process a name.
 - Confusing: In one diagram looks like hidden layer at t-1 is input to hidden layer ay t2; in the other looks like output layer at t-1 is input to hidden layer at t.
